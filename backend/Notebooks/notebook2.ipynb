{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8079778f",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "152d9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend/sitemap_parser.py\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def normalize_url(url: str) -> str:\n",
    "    url = url.lower().strip() # Remove leading/trailing whitespace and convert to lowercase\n",
    "    if not url.startswith(\"http\"):\n",
    "        url = \"https://\" + url\n",
    "    \n",
    "    if url.startswith(\"https://\") and not url.startswith(\"https://www.\"):\n",
    "        url = url.replace(\"https://\", \"https://www.\", 1)\n",
    "    elif not url.startswith(\"https://www.\"):\n",
    "        url = \"https://www.\" + url\n",
    "    \n",
    "    parsed = urlparse(url)\n",
    "\n",
    "    # Normalize to scheme + netloc only (strip path, params, query, fragment)\n",
    "    normalized_url = urlunparse((parsed.scheme, parsed.netloc, '', '', '', ''))\n",
    "    return normalized_url\n",
    "\n",
    "def fetch_sitemap(url): # Fetch each xml sitemap in one layer.\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'lxml-xml')\n",
    "        return [loc.text for loc in soup.find_all('loc')]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "def parse_sitemap(url): # Parse the sitemap and return a dictionary of URLs. Applies fetch_sitemap to each xml sitemap layer by layer.\n",
    "    locs = fetch_sitemap(url)\n",
    "    if not locs:\n",
    "        return {url: []}\n",
    "    \n",
    "    tree = {}\n",
    "    urls = []\n",
    "\n",
    "    for loc in locs:\n",
    "        if loc.endswith('.xml'):\n",
    "            tree[loc] = parse_sitemap(loc)\n",
    "        else:\n",
    "            urls.append(loc)\n",
    "\n",
    "    if tree and urls:\n",
    "        tree[\"_final_urls\"] = urls\n",
    "        return tree\n",
    "    elif urls:\n",
    "        return urls\n",
    "    else:\n",
    "        return {url: tree}\n",
    "\n",
    "def extract_final_urls(url): # List all URLs in the sitemap.\n",
    "    \n",
    "    url = normalize_url(url)\n",
    "    final_urls = [url]\n",
    "    if not url.endswith('/sitemap.xml'):\n",
    "        url += '/sitemap.xml'\n",
    "    tree = parse_sitemap(url)\n",
    "    \n",
    "\n",
    "    def _walk_tree(node):\n",
    "        if isinstance(node, dict):\n",
    "            for key, value in node.items():\n",
    "                if key == \"_final_urls\" and isinstance(value, list):\n",
    "                    final_urls.extend(value)\n",
    "                else:\n",
    "                    _walk_tree(value)\n",
    "        elif isinstance(node, list):\n",
    "            final_urls.extend([v for v in node if not v.endswith('.xml')])\n",
    "\n",
    "    _walk_tree(tree)\n",
    "\n",
    "    \n",
    "    return final_urls, tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435101c1",
   "metadata": {},
   "source": [
    "# Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "994cd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def scrape(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        if text:\n",
    "            return text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        elems = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "        text = ' '.join(elem.text for elem in elems).strip()\n",
    "        driver.quit()\n",
    "        return text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "# The output here is an input for the RAG model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacedc5f",
   "metadata": {},
   "source": [
    "# Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad7f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import json\n",
    "\n",
    "def tree_to_edges(tree, parent=None):\n",
    "    edges = []\n",
    "    if isinstance(tree, list):\n",
    "        for item in tree:\n",
    "            edges.append((parent, item))\n",
    "    elif isinstance(tree, dict):\n",
    "        for key, value in tree.items():\n",
    "            if parent:\n",
    "                edges.append((parent, key))\n",
    "            edges += tree_to_edges(value, parent=key)\n",
    "    return edges\n",
    "\n",
    "def generate_graph(sitemap_url, output_file=\"sitemap_network.html\", json_filename=None):\n",
    "    tree = parse_sitemap(sitemap_url)\n",
    "\n",
    "    # Default name if not provided\n",
    "    if json_filename is None:\n",
    "        json_filename = \"sitemap_tree.json\"\n",
    "\n",
    "    with open(json_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(tree, f, indent=2)\n",
    "\n",
    "    edges = tree_to_edges(tree[sitemap_url], parent=sitemap_url)\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "    net = Network(height=\"750px\", width=\"100%\", directed=True, notebook=False)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        if node == sitemap_url:\n",
    "            net.add_node(node, label=str(urlparse(sitemap_url).netloc), title=node, shape='dot', size=30,\n",
    "                         color={\"background\": \"white\", \"border\": \"blue\"}, borderWidth=4,\n",
    "                         font={\"color\": \"black\", \"size\": 35, \"bold\": True})\n",
    "        elif node.endswith('.xml'):\n",
    "            net.add_node(node, label=\"Sitemap\", title=node, shape='dot', size=25)\n",
    "        else:\n",
    "            net.add_node(node, label=\" \", title=node, shape='dot', size=15,\n",
    "                         color={\"background\": \"#ccffcc\", \"border\": \"#009933\"})\n",
    "\n",
    "    for source, target in G.edges():\n",
    "        net.add_edge(source, target)\n",
    "\n",
    "    net.force_atlas_2based()\n",
    "    net.set_options(\"\"\"\n",
    "    { \"physics\": { \"stabilization\": false }, \"interaction\": { \"dragNodes\": true } }\n",
    "    \"\"\")\n",
    "    net.write_html(output_file)\n",
    "\n",
    "    # Read the generated HTML\n",
    "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "\n",
    "    # Inject JS\n",
    "    inject_js = f\"\"\"\n",
    "    <script type=\"text/javascript\">\n",
    "    window.addEventListener(\"load\", function () {{\n",
    "        // âœ… Hide the Pyvis loading bar if it exists\n",
    "        const loader = document.getElementById(\"loadingBar\");\n",
    "        if (loader) loader.style.display = \"none\";\n",
    "\n",
    "        const rootNodeId = \"{sitemap_url}\";\n",
    "        const titleNodeId = \"graph_title\";\n",
    "        const originalLabels = {{}};\n",
    "\n",
    "        network.on(\"click\", function (params) {{\n",
    "        if (params.nodes.length > 0) {{\n",
    "            let clickedNodeId = params.nodes[0];\n",
    "\n",
    "            nodes.get().forEach(function (node) {{\n",
    "            if (!(node.id in originalLabels)) {{\n",
    "                originalLabels[node.id] = node.label;\n",
    "            }}\n",
    "\n",
    "            if (node.id === rootNodeId || node.id === titleNodeId) {{\n",
    "                return;\n",
    "            }}\n",
    "\n",
    "            if (node.id === clickedNodeId) {{\n",
    "                nodes.update({{id: node.id, label: node.title}});\n",
    "            }} else {{\n",
    "                nodes.update({{id: node.id, label: \"\"}});\n",
    "            }}\n",
    "            }});\n",
    "        }} else {{\n",
    "            nodes.get().forEach(function (node) {{\n",
    "            if (node.id !== rootNodeId && node.id !== titleNodeId && originalLabels[node.id] === \"\") {{\n",
    "                nodes.update({{id: node.id, label: \"\"}});\n",
    "            }}\n",
    "            }});\n",
    "        }}\n",
    "        }});\n",
    "    }});\n",
    "    </script>\n",
    "    \"\"\"\n",
    "\n",
    "    # Inject before </body>\n",
    "    html = html.replace(\"</body>\", inject_js + \"\\\\n</body>\")\n",
    "\n",
    "    # Save it back\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    print(f\"Output files successfully generated: {output_file} and {json_filename}\")\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f10af",
   "metadata": {},
   "source": [
    "# RAG model (using Langchain)\n",
    "## Database loader (run everyday to update the database based on recent information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a0ee5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openai API key successfully imported from .env file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping & indexing (Northlight AI): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34/34 [00:37<00:00,  1.09s/link]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.vectorstores import SupabaseVectorStore\n",
    "from supabase import create_client, Client\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_core.documents import Document\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "else:\n",
    "  print(f\"Openai API key successfully imported from .env file.\")\n",
    "  #print(f\"Key: {os.environ.get('OPENAI_API_KEY')}\")\n",
    "\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\")\n",
    "supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "vector_store = SupabaseVectorStore(\n",
    "    client=supabase,\n",
    "    embedding=embeddings,\n",
    "    table_name=\"prime_ready.webcontent\",  # You can change this\n",
    "    query_name=\"match_documents\"  # Needs to be created in Supabase SQL\n",
    ")\n",
    "\n",
    "def RAG_scraper_loader(company_name, website):\n",
    "\n",
    "    # delete company data from the database (both tables) - to avoid outdated data the entire company data is removed from both tables\n",
    "    supabase.rpc(\"delete_company_by_url\", {\n",
    "        \"target_url\": website\n",
    "    }).execute()\n",
    "\n",
    "    # Extract sitemap URLs\n",
    "    url_list, tree = extract_final_urls(website)\n",
    "    \n",
    "    # Add company into company table in Supabase\n",
    "    company_id = supabase.rpc(\"insert_company\", {\n",
    "        \"company_name\": company_name,\n",
    "        \"link\": website,  # âœ… must match SQL function param name\n",
    "        \"sitemap\": json.dumps(tree)  # Add the sitemap data\n",
    "\n",
    "    }).execute().data\n",
    "    \n",
    "    \n",
    "\n",
    "    for link in tqdm(url_list, desc=f\"Scraping & indexing ({company_name})\", unit=\"link\"):\n",
    "        # Scrape the URL\n",
    "        text = scrape(link)\n",
    "        if not text.strip():\n",
    "            continue  # skip empty pages\n",
    "\n",
    "        # Create metadata and document content\n",
    "        metadata={\n",
    "                \"source\": str(link),\n",
    "                \"website\": str(website),\n",
    "                \"company_id\": str(company_id)\n",
    "            }\n",
    "\n",
    "        docs = Document(\n",
    "            page_content=text,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        \n",
    "        # Chunking\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=500)\n",
    "        chunks = text_splitter.split_documents([docs])\n",
    "\n",
    "        # Index chunks and store in Supabase\n",
    "        for chunk in chunks:\n",
    "            # Generate embedding\n",
    "            vector = embeddings.embed_query(chunk.page_content)\n",
    "\n",
    "            # Insert into Supabase\n",
    "            supabase.rpc(\"insert_webcontent\", {\n",
    "                \"company_id\": company_id,\n",
    "                \"source\": link,\n",
    "                \"content\": chunk.page_content,\n",
    "                \"metadata\": metadata,\n",
    "                \"embedding\": vector  # list of floats; pgvector input accepted here\n",
    "            }).execute()\n",
    "\n",
    "company_name = \"Northlight AI\"\n",
    "website = \"https://northlightai.com\"\n",
    "RAG_scraper_loader(company_name, website)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddcf70f",
   "metadata": {},
   "source": [
    "## Step 2) Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a82b7",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1ba7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "## Prompt - custom\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use five sentences maximum and keep the answer as concise as possible.\n",
    "Always start the answer with a sentence like \"Thanks for asking question about North Light AI!\"; but be innovative and each time use a similar welcoming message.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "## State and Nodes\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "## Compile the graph\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bb800",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f16002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot():\n",
    "    question = input(\"What would you like to know? \")\n",
    "    state = graph.invoke({\"question\": question})\n",
    "    return print(state[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "376f63cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for asking about Dr. Roozbeh Ghasemi at North Light AI! He is the Lead Data & AI Analyst with a multidisciplinary background, currently pursuing a Ph.D. in Civil and Environmental Engineering. Dr. Ghasemi holds an MBA in Information Systems and Business Analytics, and his expertise includes machine learning, AI, and sustainable energy systems. He has professional experience in engineering supervision and project management, focusing on environmental systems. His work at North Light AI bridges theoretical concepts with practical applications to promote sustainable technology.\n"
     ]
    }
   ],
   "source": [
    "chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
